the best way to develop your AI skills is by building projects however sometimes figuring out what to build is the hardest part of getting started in this video I'll share five AI projects that you can build with python fast I'll break down the steps and python libraries for implementing each idea and if you're new here welcome I'm sha I make videos about data science and Entrepreneurship and if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make the number one mistake people make when thinking of project ideas is they start with the question how can I use this new technology while this can be a fine way to learn a new tool there is a better way good projects start with a different question what problem can I solve this not only makes for a better story when sharing with potential employers but solving problems is how you translate Technic Tech skills into value the following five projects all take this problem first approach you can take these ideas and Implement them directly or even better use them as inspiration for solving a problem that you face personally an effective yet timec consuming part of applying the jobs is adapting your resume to different job descriptions while automating this process would have been a pretty Advanced project a few years ago with today's large language models this this is a simple API call here's a step-by-step breakdown of how you can do this today first start by creating a markdown version of your resume note that this is something that chat GPT can do for you second go to chat GPT and experiment with different promp templates that take the markdown version of your resume and a job description and output a new resume in the markdown format third move from chat GPT to python by using open AI python API to prompt GPT 40 mini to dynamically rewrite your resume and then fourth and finally convert your markdown resume to PDF using the markdown and PDF kit python libraries while this is something we can readily use Chad GPT for the upside of implementing this process in Python is that we can easily scale it up to hundreds of resum√©s if you're having trouble getting started I have some starter code freely available in the blog post Linked In the description I'm always eager to add new technical videos to my watch later playlist however I'm usually not as eager to actually watch them so these videos tend to sit around for weeks if not months a project that could help with this is a tool that watches the videos for me and generates concise summaries with key points here's one way to do that first given a YouTube video link extract the video ID using regular Expressions Second Use the video ID to extract the transcript via the YouTube transcript API python Library third experiment with different chat GPT prompts that effectively summarize the transcript and then fourth automate this whole process by using open ai's python library from a technical perspective this is very similar to the first project a key difference however is that we will need to automatically extract video transcripts and feed them to a large language model if you want to see how to do this check out the example code in the blog post my watch later is not the only place I hoard technical information another cache is my desktop which is riddled with hundreds of unread research papers since manually reviewing these papers would be very timec consuming let's see how AI can help we can use text embeddings to translate each paper into a dense Vector representation with which similar articles can be clustered together using a traditional machine learning algorithm like K means here's a more detailed breakdown first read the abstract from each research article using the P muw PDF python library next use the sentence Transformers library to convert these abstracts into text embeddings and store them in a pandis data frame third use your favorite clustering algorithm from sklearn to group The embeddings based on similar it then fourth and finally create folders for each cluster and move each file into the appropriate folder the key step for this project is generating the text embeddings which I talked more about in this video there I talk about what they are and share example code for using them a couple of months ago I helped a company design a basic rag system to search over a set of technical reports a key challenge with searching over reports like this is that key information is often represented in plots and figures as opposed to text one way we can incorporate this visual information into the search process is using a multimodal embedding model which can represent text and images in the same embedding space here are the basic building blocks for that first given a PDF chunk it into sections and extract the images using p muw PDF Second Use a multimo embedding model to represent chunks and images as dense vectors you can find such a model on the hugging face Hub and access it using the Transformers python Library third repeat this process for all PDFs in the knowledge base fourth given a user query pass it through the same embedding model used for the knowledge base fifth compute the cosine similarity between the query embedding and every item in the knowledge base using SK learn the sixth and final step is to return the top K most similar items as search results the most important step of this project is how the PDFs are chunked the simplest way to do this is to use a fixed character count with some overlap for each chunk it's also helpful to capture metadata such as file name and page number for each chunk and image I share some basic boilerplate code for doing this in the blog post for this video over the past year I've helped about a 100 businesses and individuals build their AI projects by far the most common project people ask for is a knowledge-based question answering system this is something we can build in a straightforward way building on top of the previous project given our documents are already chunked and stored in a database we can convert the multimodal Search tool into a multimodal rag system first perform a search over the knowledge base just like we did in Project number four second combine the user query with the topk search results and pass them to a multimodal model this can be something like GPT 40 or llama 3.2 Vision third create a simple gradio user interface for the question answering system this project essentially combines projects 2 and four however it includes the essential component of a user interface for that we can use a dashboarding tool like GR radio which allows us to create a chat UI with just a few lines of code okay we covered a lot here so I wanted to close with two key takeaways the first is to start with the problem not the technology solving problems puts your effort into a larger context and is ultimately what generates value second is to use tools like chat GPT and cursor to help you be a more productive programmer issues that you used to block me for hours if not days a few years ago can now be resolved in minutes using coding assistance is the new Norm in programming so I encourage you to use these tools to help you learn faster and build Bolder projects if you have questions on any of these project ideas let me know in the comments and as always thank you so much for your time and thanks for watching